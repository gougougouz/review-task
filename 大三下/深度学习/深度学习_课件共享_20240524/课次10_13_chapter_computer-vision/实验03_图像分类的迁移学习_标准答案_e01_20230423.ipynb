{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f265b812",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# 微调\n",
    ":label:`sec_fine_tuning`\n",
    "\n",
    "\n",
    "\n",
    "## 步骤\n",
    "\n",
    "本节将介绍迁移学习中的常见技巧:*微调*（fine-tuning）。如 :numref:`fig_finetune`所示，微调包括以下四个步骤。\n",
    "\n",
    "1. 在源数据集（例如ImageNet数据集）上预训练神经网络模型，即*源模型*。\n",
    "1. 创建一个新的神经网络模型，即*目标模型*。这将复制源模型上的所有模型设计及其参数（输出层除外）。我们假定这些模型参数包含从源数据集中学到的知识，这些知识也将适用于目标数据集。我们还假设源模型的输出层与源数据集的标签密切相关；因此不在目标模型中使用该层。\n",
    "1. 向目标模型添加输出层，其输出数是目标数据集中的类别数。然后随机初始化该层的模型参数。\n",
    "1. 在目标数据集（如椅子数据集）上训练目标模型。输出层将从头开始进行训练，而所有其他层的参数将根据源模型的参数进行微调。\n",
    "\n",
    "![微调。](../img/finetune.svg)\n",
    ":label:`fig_finetune`\n",
    "\n",
    "当目标数据集比源数据集小得多时，微调有助于提高模型的泛化能力。\n",
    "\n",
    "## 热狗识别\n",
    "\n",
    "让我们通过具体案例演示微调：热狗识别。\n",
    "我们将在一个小型数据集上微调ResNet模型。该模型已在ImageNet数据集上进行了预训练。\n",
    "这个小型数据集包含数千张包含热狗和不包含热狗的图像，我们将使用微调模型来识别图像中是否包含热狗。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ff22d3e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T16:37:45.116938Z",
     "iopub.status.busy": "2022-12-07T16:37:45.116615Z",
     "iopub.status.idle": "2022-12-07T16:37:49.241932Z",
     "shell.execute_reply": "2022-12-07T16:37:49.241077Z"
    },
    "origin_pos": 2,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d138ea1",
   "metadata": {
    "origin_pos": 4
   },
   "source": [
    "### 获取数据集\n",
    "\n",
    "我们使用的[**热狗数据集来源于网络**]。\n",
    "该数据集包含1400张热狗的“正类”图像，以及包含尽可能多的其他食物的“负类”图像。\n",
    "含着两个类别的1000张图片用于训练，其余的则用于测试。\n",
    "\n",
    "解压下载的数据集，我们获得了两个文件夹`hotdog/train`和`hotdog/test`。\n",
    "这两个文件夹都有`hotdog`（有热狗）和`not-hotdog`（无热狗）两个子文件夹，\n",
    "子文件夹内都包含相应类的图像。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e960c12b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T16:37:49.246023Z",
     "iopub.status.busy": "2022-12-07T16:37:49.245639Z",
     "iopub.status.idle": "2022-12-07T16:37:57.315163Z",
     "shell.execute_reply": "2022-12-07T16:37:57.314243Z"
    },
    "origin_pos": 5,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "#@save\n",
    "d2l.DATA_HUB['hotdog'] = (d2l.DATA_URL + 'hotdog.zip',\n",
    "                         'fba480ffa8aa7e0febbb511d181409f899b9baa5')\n",
    "\n",
    "data_dir = d2l.download_extract('hotdog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d080a41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c0f79cc3",
   "metadata": {
    "origin_pos": 6
   },
   "source": [
    "我们创建两个实例来分别读取训练和测试数据集中的所有图像文件。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadf7f61",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T16:37:57.320559Z",
     "iopub.status.busy": "2022-12-07T16:37:57.320091Z",
     "iopub.status.idle": "2022-12-07T16:37:57.332319Z",
     "shell.execute_reply": "2022-12-07T16:37:57.331564Z"
    },
    "origin_pos": 8,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "train_imgs = torchvision.datasets.ImageFolder(os.path.join(data_dir, 'train'))\n",
    "test_imgs = torchvision.datasets.ImageFolder(os.path.join(data_dir, 'test'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3dbe86",
   "metadata": {
    "origin_pos": 10
   },
   "source": [
    "下面显示了前8个正类样本图片和最后8张负类样本图片。正如所看到的，[**图像的大小和纵横比各有不同**]。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd1ad03",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T16:37:57.337195Z",
     "iopub.status.busy": "2022-12-07T16:37:57.336741Z",
     "iopub.status.idle": "2022-12-07T16:37:58.109659Z",
     "shell.execute_reply": "2022-12-07T16:37:58.108824Z"
    },
    "origin_pos": 11,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "hotdogs = [train_imgs[i][0] for i in range(8)]\n",
    "not_hotdogs = [train_imgs[-i - 1][0] for i in range(8)]\n",
    "d2l.show_images(hotdogs + not_hotdogs, 2, 8, scale=1.4);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae506fa5",
   "metadata": {
    "origin_pos": 12
   },
   "source": [
    "在训练期间，我们首先从图像中裁切随机大小和随机长宽比的区域，然后将该区域缩放为$224 \\times 224$输入图像。\n",
    "在测试过程中，我们将图像的高度和宽度都缩放到256像素，然后裁剪中央$224 \\times 224$区域作为输入。\n",
    "此外，对于RGB（红、绿和蓝）颜色通道，我们分别*标准化*每个通道。\n",
    "具体而言，该通道的每个值减去该通道的平均值，然后将结果除以该通道的标准差。\n",
    "\n",
    "[~~数据增广~~]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6610d49",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T16:37:58.123563Z",
     "iopub.status.busy": "2022-12-07T16:37:58.122794Z",
     "iopub.status.idle": "2022-12-07T16:37:58.129504Z",
     "shell.execute_reply": "2022-12-07T16:37:58.128693Z"
    },
    "origin_pos": 14,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "# 使用RGB通道的均值和标准差，以标准化每个通道\n",
    "normalize = torchvision.transforms.Normalize(\n",
    "    [0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "\n",
    "train_augs = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.RandomResizedCrop(224),\n",
    "    torchvision.transforms.RandomHorizontalFlip(),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    normalize])\n",
    "\n",
    "test_augs = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize([256, 256]),\n",
    "    torchvision.transforms.CenterCrop(224),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    normalize])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be5b592",
   "metadata": {
    "origin_pos": 16
   },
   "source": [
    "### [**定义和初始化模型**]\n",
    "\n",
    "我们使用在ImageNet数据集上预训练的ResNet-18作为源模型。\n",
    "在这里，我们指定`pretrained=True`以自动下载预训练的模型参数。\n",
    "如果首次使用此模型，则需要连接互联网才能下载。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63afca77",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T16:37:58.133958Z",
     "iopub.status.busy": "2022-12-07T16:37:58.133274Z",
     "iopub.status.idle": "2022-12-07T16:37:58.407434Z",
     "shell.execute_reply": "2022-12-07T16:37:58.406531Z"
    },
    "origin_pos": 18,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "pretrained_net = torchvision.models.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26ffdfcc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T16:37:58.133958Z",
     "iopub.status.busy": "2022-12-07T16:37:58.133274Z",
     "iopub.status.idle": "2022-12-07T16:37:58.407434Z",
     "shell.execute_reply": "2022-12-07T16:37:58.406531Z"
    },
    "origin_pos": 18,
    "scrolled": true,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\software_installed\\for_work\\Anaconda3\\envs\\env_DeepLearningCourse-22-23-2\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "F:\\software_installed\\for_work\\Anaconda3\\envs\\env_DeepLearningCourse-22-23-2\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "pretrained_net = torchvision.models.vgg16(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "712a08c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace=True)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(pretrained_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8d225d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# d2l.try_all_gpus()\n",
    "a=torch.cuda.is_available()\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31546d8d",
   "metadata": {
    "origin_pos": 21,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "预训练的源模型实例包含许多特征层和一个输出层`fc`。\n",
    "此划分的主要目的是促进对除输出层以外所有层的模型参数进行微调。\n",
    "下面给出了源模型的成员变量`fc`。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bb9025",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T16:37:58.412929Z",
     "iopub.status.busy": "2022-12-07T16:37:58.412442Z",
     "iopub.status.idle": "2022-12-07T16:37:58.418165Z",
     "shell.execute_reply": "2022-12-07T16:37:58.417415Z"
    },
    "origin_pos": 24,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "pretrained_net.fc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585783fb",
   "metadata": {
    "origin_pos": 25
   },
   "source": [
    "在ResNet的全局平均汇聚层后，全连接层转换为ImageNet数据集的1000个类输出。\n",
    "之后，我们构建一个新的神经网络作为目标模型。\n",
    "它的定义方式与预训练源模型的定义方式相同，只是最终层中的输出数量被设置为目标数据集中的类数（而不是1000个）。\n",
    "\n",
    "在下面的代码中，目标模型`finetune_net`中成员变量`features`的参数被初始化为源模型相应层的模型参数。\n",
    "由于模型参数是在ImageNet数据集上预训练的，并且足够好，因此通常只需要较小的学习率即可微调这些参数。\n",
    "\n",
    "成员变量`output`的参数是随机初始化的，通常需要更高的学习率才能从头开始训练。\n",
    "假设`Trainer`实例中的学习率为$\\eta$，我们将成员变量`output`中参数的学习率设置为$10\\eta$。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43500069",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T16:37:58.422637Z",
     "iopub.status.busy": "2022-12-07T16:37:58.422074Z",
     "iopub.status.idle": "2022-12-07T16:37:58.669492Z",
     "shell.execute_reply": "2022-12-07T16:37:58.668626Z"
    },
    "origin_pos": 27,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "finetune_net = torchvision.models.resnet18(pretrained=True)\n",
    "finetune_net.fc = nn.Linear(finetune_net.fc.in_features, 2)\n",
    "nn.init.xavier_uniform_(finetune_net.fc.weight);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49531348",
   "metadata": {
    "origin_pos": 29
   },
   "source": [
    "### [**微调模型**]\n",
    "\n",
    "首先，我们定义了一个训练函数`train_fine_tuning`，该函数使用微调，因此可以多次调用。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b3e101",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T16:37:58.674339Z",
     "iopub.status.busy": "2022-12-07T16:37:58.673792Z",
     "iopub.status.idle": "2022-12-07T16:37:58.681528Z",
     "shell.execute_reply": "2022-12-07T16:37:58.680703Z"
    },
    "origin_pos": 31,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "# 如果param_group=True，输出层中的模型参数将使用十倍的学习率\n",
    "def train_fine_tuning(net, learning_rate, batch_size=128, num_epochs=5,\n",
    "                      param_group=True):\n",
    "    train_iter = torch.utils.data.DataLoader(torchvision.datasets.ImageFolder(\n",
    "        os.path.join(data_dir, 'train'), transform=train_augs),\n",
    "        batch_size=batch_size, shuffle=True)\n",
    "    test_iter = torch.utils.data.DataLoader(torchvision.datasets.ImageFolder(\n",
    "        os.path.join(data_dir, 'test'), transform=test_augs),\n",
    "        batch_size=batch_size)\n",
    "    devices = d2l.try_all_gpus()\n",
    "    loss = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "    if param_group:\n",
    "        params_1x = [param for name, param in net.named_parameters()\n",
    "             if name not in [\"fc.weight\", \"fc.bias\"]]\n",
    "        trainer = torch.optim.SGD([{'params': params_1x},\n",
    "                                   {'params': net.fc.parameters(),\n",
    "                                    'lr': learning_rate * 10}],\n",
    "                                lr=learning_rate, weight_decay=0.001)\n",
    "    else:\n",
    "        trainer = torch.optim.SGD(net.parameters(), lr=learning_rate,\n",
    "                                  weight_decay=0.001)\n",
    "    d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs,\n",
    "                   devices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0944654",
   "metadata": {
    "origin_pos": 33
   },
   "source": [
    "我们[**使用较小的学习率**]，通过*微调*预训练获得的模型参数。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca4ef19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T16:37:58.685559Z",
     "iopub.status.busy": "2022-12-07T16:37:58.684806Z",
     "iopub.status.idle": "2022-12-07T16:39:23.696907Z",
     "shell.execute_reply": "2022-12-07T16:39:23.696074Z"
    },
    "origin_pos": 35,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "train_fine_tuning(finetune_net, 5e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a04d31e",
   "metadata": {
    "origin_pos": 36
   },
   "source": [
    "[**为了进行比较，**]我们定义了一个相同的模型，但是将其(**所有模型参数初始化为随机值**)。\n",
    "由于整个模型需要从头开始训练，因此我们需要使用更大的学习率。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72ea239",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T16:39:23.702375Z",
     "iopub.status.busy": "2022-12-07T16:39:23.701792Z",
     "iopub.status.idle": "2022-12-07T16:40:45.446588Z",
     "shell.execute_reply": "2022-12-07T16:40:45.445443Z"
    },
    "origin_pos": 38,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "scratch_net = torchvision.models.resnet18()\n",
    "scratch_net.fc = nn.Linear(scratch_net.fc.in_features, 2)\n",
    "train_fine_tuning(scratch_net, 5e-4, param_group=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e371a53",
   "metadata": {
    "origin_pos": 40
   },
   "source": [
    "意料之中，微调模型往往表现更好，因为它的初始参数值更有效。\n",
    "\n",
    "## 小结\n",
    "\n",
    "* 迁移学习将从源数据集中学到的知识*迁移*到目标数据集，微调是迁移学习的常见技巧。\n",
    "* 除输出层外，目标模型从源模型中复制所有模型设计及其参数，并根据目标数据集对这些参数进行微调。但是，目标模型的输出层需要从头开始训练。\n",
    "* 通常，微调参数使用较小的学习率，而从头开始训练输出层可以使用更大的学习率。\n",
    "\n",
    "## 练习\n",
    "\n",
    "1. 继续提高`finetune_net`的学习率，模型的准确性如何变化？\n",
    "2. 在比较实验中进一步调整`finetune_net`和`scratch_net`的超参数。它们的准确性还有不同吗？\n",
    "3. 将输出层`finetune_net`之前的参数设置为源模型的参数，在训练期间不要更新它们。模型的准确性如何变化？提示：可以使用以下代码。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19433761",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T16:40:45.450383Z",
     "iopub.status.busy": "2022-12-07T16:40:45.449520Z",
     "iopub.status.idle": "2022-12-07T16:40:45.455316Z",
     "shell.execute_reply": "2022-12-07T16:40:45.453890Z"
    },
    "origin_pos": 42,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "for param in finetune_net.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9107a0",
   "metadata": {
    "origin_pos": 44
   },
   "source": [
    "4. 事实上，`ImageNet`数据集中有一个“热狗”类别。我们可以通过以下代码获取其输出层中的相应权重参数，但是我们怎样才能利用这个权重参数？\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03889099",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T16:40:45.460528Z",
     "iopub.status.busy": "2022-12-07T16:40:45.459409Z",
     "iopub.status.idle": "2022-12-07T16:40:45.472096Z",
     "shell.execute_reply": "2022-12-07T16:40:45.470704Z"
    },
    "origin_pos": 46,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "weight = pretrained_net.fc.weight\n",
    "hotdog_w = torch.split(weight.data, 1, dim=0)[934]\n",
    "hotdog_w.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72aa964d",
   "metadata": {
    "origin_pos": 49,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "[Discussions](https://discuss.d2l.ai/t/2894)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
